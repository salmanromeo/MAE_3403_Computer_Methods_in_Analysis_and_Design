{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNT0Y8MY7/XBnSl30QWev6z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmanromeo/MAE_3403_Computer_Methods_in_Analysis_and_Design/blob/main/lecture_9_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization (scipy.optimize)**"
      ],
      "metadata": {
        "id": "tyCwdHY1uHvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####The **scipy.optimize** package provides several commonly used optimization algorithms."
      ],
      "metadata": {
        "id": "kTAYZIX5uL2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unconstrained minimization of multivariate scalar functions (minimize)**"
      ],
      "metadata": {
        "id": "3Pm5CKQVvE06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####The **minimize** function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in scipy.optimize. To demonstrate the minimization function, consider the problem of minimizing the **Rosenbrock function** of $N$ variables:\n",
        "\\begin{align}\n",
        "  f(x) = \\sum_{i=1}^{N-1} 100(x_{i+1}-x_i^2)^2 + (1-x_i)^2\n",
        "    \\end{align}\n",
        "#####The minimum value of this function is $0$ which is achieved when $x_i=1$."
      ],
      "metadata": {
        "id": "vaA9ly33vmta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1a. Nelder-Mead Simplex algorithm (method='Nelder-Mead')**"
      ],
      "metadata": {
        "id": "b_omFWR3wtTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR2NPBrmtqjm",
        "outputId": "ad87c513-ea43-422b-f3de-b6b778aa2cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 339\n",
            "         Function evaluations: 571\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the Rosenbrock function\n",
        "def rosen(x):\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "\n",
        "res = minimize(rosen, x0, method='nelder-mead',\n",
        "               options={'xatol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####The simplex algorithm is probably the simplest way to minimize a fairly well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum."
      ],
      "metadata": {
        "id": "DpiHBu0qxpIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1b. Nelder-Mead Simplex algorithm (method='powell')**"
      ],
      "metadata": {
        "id": "QXSG6ofpyef1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Another optimization algorithm that needs only function calls to find the minimum is Powell‚Äôs method available by setting **method='powell'** in minimize."
      ],
      "metadata": {
        "id": "ccI1IeO_y4TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the Rosenbrock function\n",
        "def rosen(x):\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "\n",
        "res = minimize(rosen, x0, method='powell',\n",
        "               options={'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loaswD6syIYh",
        "outputId": "c6ccfae4-1137-420c-9df1-7b037ea7d239"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 18\n",
            "         Function evaluations: 988\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1c. Rosenbrock function with an additional scaling factor $a$ and an offset $b$**"
      ],
      "metadata": {
        "id": "8EkSLhk8xyUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "  f(x,a,b) = \\sum_{i=1}^{N-1} a(x_{i+1}-x_i^2)^2 + (1-x_i)^2 + b\n",
        "    \\end{align}\n",
        "#####Example parameters $a=0.5$ and $b=1$."
      ],
      "metadata": {
        "id": "zf5V3-g8xyro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the Rosenbrock function with additional arguments\n",
        "def rosen_with_args(x, a, b):\n",
        "    return sum(a*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0) + b\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "\n",
        "res = minimize(rosen_with_args, x0, method='nelder-mead',\n",
        "               args=(0.5, 1.), options={'xatol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCHRVQrGwx0Z",
        "outputId": "2ef35275-7ebf-48fa-9e89-fe7b1e34ae20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 1.000000\n",
            "         Iterations: 319\n",
            "         Function evaluations: 525\n",
            "[1.         1.         1.         1.         0.99999999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2a. Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS')**"
      ],
      "metadata": {
        "id": "elQLIEDfz-JO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####In order to converge more quickly to the solution, this routine uses the gradient of the objective function.\n",
        "#####To demonstrate this algorithm, the Rosenbrock function is again used. The gradient of the Rosenbrock function is the vector:\n",
        "\\begin{align}\n",
        "  \\frac{‚àÇf}{‚àÇx_j} = \\sum_{i=1}^{N} 200(x_{i}-x_{i-1}^2)(Œ¥_{i,j}-2x_{i-1}Œ¥_{i-1,j}) - 2(1-x_{i-1})Œ¥_{i-1,j} \\\\[1em]\n",
        "  = 200(x_{j}-x_{j-1}^2) -400x_j(x_{j+1}-x_j^2)- 2(1-x_{j})\n",
        "    \\end{align}\n",
        "#####This expression is valid for the interior derivatives. Special cases are\n",
        "\\begin{align}\n",
        "  \\frac{‚àÇf}{‚àÇx_0} = -400x_0(x_1-x_0^2)- 2(1-x_{0}) \\\\[1em]\n",
        "    \\frac{‚àÇf}{‚àÇx_{N-1}} = 200x_0(x_{N-1}-x_{N-2}^2)\n",
        "    \\end{align}"
      ],
      "metadata": {
        "id": "LRAvbmzF0HOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the Rosenbrock function\n",
        "def rosen(x):\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "# define derivatives\n",
        "def rosen_der(x):\n",
        "    xm = x[1:-1]\n",
        "    xm_m1 = x[:-2]\n",
        "    xm_p1 = x[2:]\n",
        "    der = np.zeros_like(x)\n",
        "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
        "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
        "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
        "    return der\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "\n",
        "res = minimize(rosen, x0, tol = 1e-8, method='BFGS', jac=rosen_der,\n",
        "               options={'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PuoMlQn0BLP",
        "outputId": "f0459602-91fa-46c4-bd4d-c1e1a5329ee7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 28\n",
            "         Function evaluations: 33\n",
            "         Gradient evaluations: 33\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2b. Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS', jac=True)**"
      ],
      "metadata": {
        "id": "Rf8uQJpA4zpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Another way to supply gradient information is to write a single function which returns both the objective and the gradient: this is indicated by setting **jac=True**.\n",
        "#####In this case, the Python function to be optimized must return a tuple whose first value is the objective and whose second value represents the gradient."
      ],
      "metadata": {
        "id": "XtlKNPRe5GYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the Rosenbrock function\n",
        "def rosen(x):\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "# define derivatives\n",
        "def rosen_and_der(x):\n",
        "    objective = sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "    xm = x[1:-1]\n",
        "    xm_m1 = x[:-2]\n",
        "    xm_p1 = x[2:]\n",
        "    der = np.zeros_like(x)\n",
        "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
        "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
        "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
        "    return objective, der\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "\n",
        "res = minimize(rosen_and_der, x0, tol = 1e-8, method='BFGS', jac=True,\n",
        "               options={'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7zGIjKE5JKq",
        "outputId": "557d5b43-a037-440a-ae5f-b59bf6c3e837"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 28\n",
            "         Function evaluations: 33\n",
            "         Gradient evaluations: 33\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Supplying objective and gradient in a single function can help to avoid redundant computations and therefore speed up the optimization significantly."
      ],
      "metadata": {
        "id": "pdfLSdTc5eCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Newton-Conjugate-Gradient algorithm (method='Newton-CG')**"
      ],
      "metadata": {
        "id": "XQs0O7bk5hlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Newton-Conjugate Gradient algorithm is a modified Newton‚Äôs method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian. Newton‚Äôs method is based on fitting the function locally to a quadratic form:\n",
        "\\begin{align}\n",
        "  f(x) ‚âà f(x_0)+‚àáf(x_0)(x-x_0)+\\frac{1}{2}(x-x_0)^Tùêá(x_0)(x-x_0)\n",
        "    \\end{align}\n",
        "#####where $ùêá(x_0)$ is a matrix of second-derivatives (the Hessian). If the Hessian is positive definite then the local minimum of this function can be found by setting the gradient of the quadratic form to zero, resulting in\n",
        "\\begin{align}\n",
        "  x_{opt} = x_0-ùêá^{-1}‚àáf\n",
        "    \\end{align}\n",
        "#####The inverse of the Hessian is evaluated using the conjugate-gradient method. An example of employing this method to minimizing the Rosenbrock function is given below.\n",
        "#####The Hessian of the Rosenbrock function is\n",
        "\\begin{align}\n",
        "  ùêá_{i,j} = \\frac{‚àÇ^2f}{‚àÇx_i‚àÇx_j} = 200(Œ¥_{i,j}-2x_{i-1}Œ¥_{i-1,j})-400x_i(‚àÇ_{i+1,j}-2x_iŒ¥_{i,j})-400Œ¥_{i,j}(x_{i+1}-x_{i}^2)+2Œ¥_{i,j} \\\\[1em]\n",
        "  =(202+1200x_{i}^2-400x_{i+1})Œ¥_{i,j}-400x_iŒ¥_{i+1,j}-400x_{i-1}Œ¥_{i-1,j}\n",
        "    \\end{align}\n",
        "#####if $i,j ‚àà [1,N-2]$ with $i,j ‚àà [0,N-1]$ defining the $NxN$ matrix. Other non-zero entries of the matrix are\n",
        "\\begin{align}\n",
        "  \\frac{‚àÇ^2f}{‚àÇx_0^2} = 1200x_0-400x_1+2; \\\\[1em]\n",
        "  \\frac{‚àÇ^2f}{‚àÇx_0‚àÇx_1} = \\frac{‚àÇ^2f}{‚àÇx_1‚àÇx_0} =-400x_0; \\\\[1em]\n",
        "  \\frac{‚àÇ^2f}{‚àÇx_{N-1}‚àÇx_{N-2}} = \\frac{‚àÇ^2f}{‚àÇx_{N-2}‚àÇx_{N-1}} =-400x_{N-2};  \\\\[1em]\n",
        "  \\frac{‚àÇ^2f}{‚àÇx_{N-1}^2} = 200.\n",
        "    \\end{align}\n"
      ],
      "metadata": {
        "id": "lZft16zo5op-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####For example, the Hessian when $N=5$ is\n",
        "\\begin{align}\n",
        "ùêá = \\begin{bmatrix}\n",
        "1200x_0-400x_1+2 & -400x_0 & 0 & 0 & 0\\\\\n",
        "-400x_0 & 202+1200x_1^2-400x_2 & -400x_1 & 0 & 0\\\\\n",
        "0 & -400x_1 & 202+1200x_2^2-400x_3 & -400x_2 & 0\\\\\n",
        "0 & 0 & -400x_2 & 202+1200x_3^2-400x_4 & -400x_3 \\\\\n",
        "0 & 0 & 0 & -400x_3 & 200\n",
        "\\end{bmatrix}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "CJn3VXVz_Tcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####The code which computes this Hessian along with the code to minimize the function using Newton-CG method is shown in the following example:"
      ],
      "metadata": {
        "id": "g3Q-K-t9BLKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the Rosenbrock function\n",
        "def rosen(x):\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "# define derivatives\n",
        "def rosen_der(x):\n",
        "    xm = x[1:-1]\n",
        "    xm_m1 = x[:-2]\n",
        "    xm_p1 = x[2:]\n",
        "    der = np.zeros_like(x)\n",
        "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
        "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
        "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
        "    return der\n",
        "\n",
        "# define Hessian\n",
        "def rosen_hess(x):\n",
        "    x = np.asarray(x)\n",
        "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
        "    diagonal = np.zeros_like(x)\n",
        "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
        "    diagonal[-1] = 200\n",
        "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
        "    H = H + np.diag(diagonal)\n",
        "    return H\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "\n",
        "res = minimize(rosen, x0, method='Newton-CG',\n",
        "               jac=rosen_der, hess=rosen_hess,\n",
        "               options={'xtol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeopnAA4BQ5R",
        "outputId": "ae3c228e-f73f-492a-da8f-ee2af077c0a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 24\n",
            "         Function evaluations: 33\n",
            "         Gradient evaluations: 33\n",
            "         Hessian evaluations: 24\n",
            "[1.         1.         1.         0.99999999 0.99999999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')**"
      ],
      "metadata": {
        "id": "OL4IHlfwCfKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####The Newton-CG method is a line search method: it finds a direction of search minimizing a quadratic approximation of the function and then uses a line search algorithm to find the (nearly) optimal step size in that direction. An alternative approach is to, first, fix the step size limit $ùö´$ and then find the optimal step $p$ inside the given trust-radius by solving the following quadratic subproblem:\n",
        "\\begin{align}\n",
        "  \\text{min}_{p} f(x_k) + ‚àáf(x_k).p+\\frac{1}{2}p^Tùêá(x_k)p \\\\[1em]\n",
        "  \\text{subject to:} \\mid\\mid p\\mid\\mid \\le ùö´\n",
        "    \\end{align}\n",
        "#####The solution is then updated \\begin{align}\n",
        "  x_{k+1} = x_k+p\n",
        "    \\end{align}\n",
        " and the trust-radius $ùö´$ is adjusted according to the degree of agreement of the quadratic model with the real function. This family of methods is known as trust-region methods. The trust-ncg algorithm is a trust-region method that uses a conjugate gradient algorithm to solve the trust-region subproblem."
      ],
      "metadata": {
        "id": "k934sACjChCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the Rosenbrock function\n",
        "def rosen(x):\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "# define derivatives\n",
        "def rosen_der(x):\n",
        "    xm = x[1:-1]\n",
        "    xm_m1 = x[:-2]\n",
        "    xm_p1 = x[2:]\n",
        "    der = np.zeros_like(x)\n",
        "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
        "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
        "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
        "    return der\n",
        "\n",
        "# define Hessian\n",
        "def rosen_hess(x):\n",
        "    x = np.asarray(x)\n",
        "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
        "    diagonal = np.zeros_like(x)\n",
        "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
        "    diagonal[-1] = 200\n",
        "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
        "    H = H + np.diag(diagonal)\n",
        "    return H\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "\n",
        "res = minimize(rosen, x0, method='trust-ncg',\n",
        "               jac=rosen_der, hess=rosen_hess,\n",
        "               options={'gtol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFLLN2mSD7NB",
        "outputId": "69eeeba3-2ef4-44a2-ca55-c3f1a56c4824"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 20\n",
            "         Function evaluations: 21\n",
            "         Gradient evaluations: 20\n",
            "         Hessian evaluations: 19\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constrained minimization of multivariate scalar functions (minimize)**"
      ],
      "metadata": {
        "id": "Tf5GMTVbvIZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####As an example let us consider the constrained minimization of the Rosenbrock function:\n",
        "\\begin{align}\n",
        "  \\text{min}_{x_0,x_1} 100(x_1-x_0^2)^2 + (1-x_0)^2\\\\[1em]\n",
        "  \\text{subject to:} \\\\[1em]\n",
        "  x_0+2x_1 ‚â§ 1 \\\\[1em]\n",
        "  x_0^2+x_1 ‚â§ 1 \\\\[1em]\n",
        "  x_0^2-x_1 ‚â§ 1 \\\\[1em]\n",
        "  2x_0+x_1 = 1 \\\\[1em]\n",
        "  0 \\le x_0 \\le 1 \\\\[1em]\n",
        "  -0.5 \\le x_1 \\le 2\n",
        "    \\end{align}\n",
        "#####This optimization problem has the unique solution \\begin{align}\n",
        "[x_0,x_1] = [0.4149,0.1701]\n",
        "    \\end{align}\n",
        "for which only the first and fourth constraints are active."
      ],
      "metadata": {
        "id": "wDN4iiYHEfPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trust-Region Constrained Algorithm (method='trust-constr')**"
      ],
      "metadata": {
        "id": "-W3MXvk3HL9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "from scipy.optimize import Bounds\n",
        "from scipy.optimize import LinearConstraint\n",
        "from scipy.optimize import NonlinearConstraint\n",
        "\n",
        "# define the Rosenbrock function\n",
        "def rosen(x):\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "# define derivatives\n",
        "def rosen_der(x):\n",
        "    xm = x[1:-1]\n",
        "    xm_m1 = x[:-2]\n",
        "    xm_p1 = x[2:]\n",
        "    der = np.zeros_like(x)\n",
        "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
        "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
        "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
        "    return der\n",
        "\n",
        "# define Hessian\n",
        "def rosen_hess(x):\n",
        "    x = np.asarray(x)\n",
        "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
        "    diagonal = np.zeros_like(x)\n",
        "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
        "    diagonal[-1] = 200\n",
        "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
        "    H = H + np.diag(diagonal)\n",
        "    return H\n",
        "\n",
        "# define an initial guess for the optimization algorithm\n",
        "x0 = np.array([0.5, 0])\n",
        "\n",
        "# define bounds constraints\n",
        "bounds = Bounds([0, -0.5], [1.0, 2.0])\n",
        "\n",
        "# define linear constraints\n",
        "linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1], [1, 1])\n",
        "\n",
        "# define nonlinear constraints\n",
        "def cons_f(x):\n",
        "    return [x[0]**2 + x[1], x[0]**2 - x[1]]\n",
        "def cons_J(x):\n",
        "    return [[2*x[0], 1], [2*x[0], -1]]\n",
        "def cons_H(x, v):\n",
        "    return v[0]*np.array([[2, 0], [0, 0]]) + v[1]*np.array([[2, 0], [0, 0]])\n",
        "\n",
        "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=cons_H)\n",
        "\n",
        "res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess,\n",
        "               constraints=[linear_constraint, nonlinear_constraint],\n",
        "               options={'verbose': 2}, bounds=bounds)\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tQcxcx0vJrF",
        "outputId": "266e5640-df15-4a84-fe57-1158a3c87f8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  |\n",
            "|-------|-------|-------|-------------|----------|----------|----------|\n",
            "|   1   |   1   |   0   | +6.5000e+00 | 1.00e+00 | 1.05e+01 | 0.00e+00 |\n",
            "|   2   |   2   |   1   | +9.3930e-01 | 6.13e+00 | 1.23e+00 | 1.27e-01 |\n",
            "|   3   |   3   |   2   | +3.8755e-01 | 6.13e+00 | 1.38e-01 | 1.11e-16 |\n",
            "|   4   |   4   |   3   | +3.4360e-01 | 6.13e+00 | 7.72e-03 | 0.00e+00 |\n",
            "|   5   |   4   |   3   | +3.4360e-01 | 3.07e+01 | 1.64e-02 | 0.00e+00 |\n",
            "|   6   |   5   |   4   | +3.4273e-01 | 3.07e+01 | 1.60e-04 | 0.00e+00 |\n",
            "|   7   |   5   |   4   | +3.4273e-01 | 1.53e+02 | 1.89e-03 | 0.00e+00 |\n",
            "|   8   |   6   |   5   | +3.4272e-01 | 1.53e+02 | 2.21e-06 | 0.00e+00 |\n",
            "|   9   |   6   |   5   | +3.4272e-01 | 7.67e+02 | 3.48e-04 | 0.00e+00 |\n",
            "|  10   |   7   |   6   | +3.4272e-01 | 7.67e+02 | 7.54e-08 | 0.00e+00 |\n",
            "|  11   |   7   |   6   | +3.4272e-01 | 3.83e+03 | 6.92e-05 | 0.00e+00 |\n",
            "|  12   |   8   |   7   | +3.4272e-01 | 3.83e+03 | 2.99e-09 | 0.00e+00 |\n",
            "\n",
            "`gtol` termination condition is satisfied.\n",
            "Number of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 0.00e+00, execution time: 0.033 s.\n",
            "[0.41494531 0.17010937]\n"
          ]
        }
      ]
    }
  ]
}